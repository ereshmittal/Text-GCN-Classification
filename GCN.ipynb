{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQCF7G3iX9aXAJLXULSmfM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ereshmittal/Text-GCN-Classification/blob/main/GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "from argparse import ArgumentParser\n",
        "from collections import OrderedDict\n",
        "import random\n",
        "from random import shuffle\n",
        "from itertools import combinations\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieWR37h_1eqt",
        "outputId": "1d8d0885-80bc-4f69-8398-292eeeae90b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "7WfpAQ1Ex7If",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "137e7c0c-16c5-4cdb-95bd-b15155ef474a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_bbe_url = \"https://raw.githubusercontent.com/plkmo/Bible_Text_GCN/master/data/t_bbe.csv\"\n",
        "key_url = \"https://raw.githubusercontent.com/plkmo/Bible_Text_GCN/master/data/key_english.csv\"\n",
        "bbe = pd.read_csv(t_bbe_url)\n",
        "key = pd.read_csv(key_url)"
      ],
      "metadata": {
        "id": "F9V3rbfJxl62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbe.drop(['id', 'v'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "WZd3nskByHQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nCr(n,r):\n",
        "    f = math.factorial\n",
        "    return int(f(n)/(f(r)*f(n-r)))\n",
        "\n",
        "def word_word_edges(p_ij):\n",
        "    word_word = []\n",
        "    cols = list(p_ij.columns); cols = [str(w) for w in cols]\n",
        "\n",
        "    for w1, w2 in tqdm(combinations(cols, 2), total=nCr(len(cols), 2)):\n",
        "        if (p_ij.loc[w1,w2] > 0):\n",
        "            word_word.append((w1,w2,{\"weight\":p_ij.loc[w1,w2]}))\n",
        "    return word_word"
      ],
      "metadata": {
        "id": "y-nBuNwxuLbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bookmap = {book.lower(): number for number, book in zip(key['field'], key['field.1'])}"
      ],
      "metadata": {
        "id": "JISfvWZPvW7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = pd.DataFrame(columns=['b', 'c'])\n",
        "for book in bbe['b'].unique():\n",
        "  dum = pd.DataFrame()\n",
        "  dum['c'] = (bbe[bbe['b'] == book].groupby('c')).apply(lambda x: (\" \".join(x[\"t\"])).lower())\n",
        "  dum['b'] = book\n",
        "  df_data = pd.concat([df_data, dum], ignore_index=True)\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word not in stopwords]\n",
        "    return (\" \".join(filtered_text))\n",
        "\n",
        "def stemming(text):\n",
        "    porter = PorterStemmer()\n",
        "    \n",
        "    result=[]\n",
        "    for word in text:\n",
        "        result.append(porter.stem(word))\n",
        "    return (\"\".join(result))\n",
        "\n",
        "\n",
        "df_data['c'] = df_data['c'].apply(remove_punctuation)\n",
        "df_data['c'] = df_data['c'].apply(remove_stopwords)\n",
        "df_data['c'] = df_data['c'].apply(stemming)"
      ],
      "metadata": {
        "id": "gaoIT1hCwN2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data['c']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5iAx4wIviV-",
        "outputId": "c06f7b3d-a621-4a9f-abd9-73a54336716d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       first god made heaven earth earth waste withou...\n",
              "1       heaven earth things complete seventh day god c...\n",
              "2       snake wiser beast field lord god made said wom...\n",
              "3       man connection eve wife became child gave birt...\n",
              "4       book generations adam day god made man made im...\n",
              "                              ...                        \n",
              "1184    things saw another angel coming heaven great a...\n",
              "1185    things came ears sound like voice great band p...\n",
              "1186    saw angel coming heaven key great deep great c...\n",
              "1187    saw new heaven new earth first heaven first ea...\n",
              "1188    saw river water life clear glass coming high s...\n",
              "Name: c, Length: 1189, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(input=\"content\", max_features=None, lowercase=False)\n",
        "vectorizer.fit(df_data[\"c\"])\n",
        "df_tfidf = vectorizer.transform(df_data[\"c\"])\n",
        "df_tfidf = df_tfidf.toarray()\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "vocab = np.array(vocab)\n",
        "df_tfidf = pd.DataFrame(df_tfidf,columns=vocab)"
      ],
      "metadata": {
        "id": "afsHwxYv1FB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/codeKgu/Text-GCN\n",
        "# https://github.com/yao8839836/text_gcn\n",
        "# https://github.com/iworldtong/text_gcn.pytorch\n",
        "# https://github.com/andrejmiscic/gcn-pytorch\n",
        "# https://towardsdatascience.com/text-based-graph-convolutional-network-for-semi-supervised-bible-book-classification-c71f6f61ff0f\n",
        "# https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html\n",
        "# https://paperswithcode.com/paper/graph-convolutional-networks-for-text"
      ],
      "metadata": {
        "id": "bT9XCbb7Z9ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = np.array(vocab)"
      ],
      "metadata": {
        "id": "o24N9DkICOqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {name: index for (index, name) in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "TS9cLhcIcXc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "occurrences = np.zeros((len(vocab),len(vocab)), dtype=np.int32)"
      ],
      "metadata": {
        "id": "QTHtA2zJ5gCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_i  = OrderedDict((name, 0) for name in vocab)"
      ],
      "metadata": {
        "id": "Ua3EB1jO7A6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data['c'] = df_data['c'].map(lambda x: word_tokenize(x))\n",
        "df_data['c']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb3bQs3E98l3",
        "outputId": "b16356ac-6e9b-4620-ce62-aa4343611f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [first, god, made, heaven, earth, earth, waste...\n",
              "1       [heaven, earth, things, complete, seventh, day...\n",
              "2       [snake, wiser, beast, field, lord, god, made, ...\n",
              "3       [man, connection, eve, wife, became, child, ga...\n",
              "4       [book, generations, adam, day, god, made, man,...\n",
              "                              ...                        \n",
              "1184    [things, saw, another, angel, coming, heaven, ...\n",
              "1185    [things, came, ears, sound, like, voice, great...\n",
              "1186    [saw, angel, coming, heaven, key, great, deep,...\n",
              "1187    [saw, new, heaven, new, earth, first, heaven, ...\n",
              "1188    [saw, river, water, life, clear, glass, coming...\n",
              "Name: c, Length: 1189, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window=10\n",
        "no_windows = 0\n",
        "\n",
        "for l in tqdm(df_data[\"c\"], total=len(df_data[\"c\"])):\n",
        "    for i in range(len(l)-window):\n",
        "      d = set(l[i: (i+window)])\n",
        "\n",
        "      for w in d:\n",
        "        n_i[w] += 1\n",
        "      for w1,w2 in combinations(d,2):\n",
        "        i1 = word2idx[w1]\n",
        "        i2 = word2idx[w2]\n",
        "        occurrences[i1][i2] +=1 \n",
        "        occurrences[i2][i1] +=1 \n",
        "\n",
        "p_ij = pd.DataFrame(occurrences, index = vocab,columns=vocab)/no_windows\n",
        "p_i = pd.Series(n_i, index=n_i.keys())/no_windows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuOTwTnP50YI",
        "outputId": "7b06c03c-ebb6-4ffb-dc36-3ea106ed696f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1189/1189 [01:38<00:00, 12.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in p_ij.columns:\n",
        "       p_ij[col] = p_ij[col]/p_i[col]\n",
        "\n",
        "for row in p_ij.index:\n",
        "    p_ij.loc[row,:] = p_ij.loc[row,:]/p_i[row]\n",
        "\n",
        "p_ij = p_ij + 1E-9\n",
        "\n",
        "for col in p_ij.columns:\n",
        "    p_ij[col] = p_ij[col].apply(lambda x: math.log(x))"
      ],
      "metadata": {
        "id": "9Fg6rNAl6ufS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.Graph()"
      ],
      "metadata": {
        "id": "yrAK95SyEPQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G.add_nodes_from(df_tfidf.index)\n",
        "G.add_nodes_from(vocab)"
      ],
      "metadata": {
        "id": "T2vyddgBFWzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_word = word_word_edges(p_ij)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0fNf5XZFm1Q",
        "outputId": "d193c760-fb01-4c79-dc87-376897819035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22913065/22913065 [03:17<00:00, 115868.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_word = [(doc,w,{\"weight\":df_tfidf.loc[doc,w]}) for doc in tqdm(df_tfidf.index, total=len(df_tfidf.index))\\\n",
        "                 for w in df_tfidf.columns]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywDprkvXF3du",
        "outputId": "bc003146-1622-4c4c-e913-749b4c8684b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1189/1189 [01:16<00:00, 15.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G.add_edges_from(document_word)\n",
        "G.add_edges_from(word_word)"
      ],
      "metadata": {
        "id": "r0CWkLFIGAq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class gcn(nn.Module):\n",
        "    def __init__(self, X_size, A_hat, args, bias=True): # X_size = num features\n",
        "        super(gcn, self).__init__()\n",
        "        self.A_hat = torch.tensor(A_hat, requires_grad=False).float()\n",
        "        self.weight = nn.parameter.Parameter(torch.FloatTensor(X_size, args.hidden_size_1))\n",
        "        var = 2./(self.weight.size(1)+self.weight.size(0))\n",
        "        self.weight.data.normal_(0,var)\n",
        "        self.weight2 = nn.parameter.Parameter(torch.FloatTensor(args.hidden_size_1, args.hidden_size_2))\n",
        "        var2 = 2./(self.weight2.size(1)+self.weight2.size(0))\n",
        "        self.weight2.data.normal_(0,var2)\n",
        "        if bias:\n",
        "            self.bias = nn.parameter.Parameter(torch.FloatTensor(args.hidden_size_1))\n",
        "            self.bias.data.normal_(0,var)\n",
        "            self.bias2 = nn.parameter.Parameter(torch.FloatTensor(args.hidden_size_2))\n",
        "            self.bias2.data.normal_(0,var2)\n",
        "        else:\n",
        "            self.register_parameter(\"bias\", None)\n",
        "        self.fc1 = nn.Linear(args.hidden_size_2, args.num_classes)\n",
        "        \n",
        "    def forward(self, X): ### 2-layer GCN architecture\n",
        "        X = torch.mm(X, self.weight)\n",
        "        if self.bias is not None:\n",
        "            X = (X + self.bias)\n",
        "        X = F.relu(torch.mm(self.A_hat, X))\n",
        "        X = torch.mm(X, self.weight2)\n",
        "        if self.bias2 is not None:\n",
        "            X = (X + self.bias2)\n",
        "        X = F.relu(torch.mm(self.A_hat, X))\n",
        "        return self.fc1(X)"
      ],
      "metadata": {
        "id": "NUSSKezRGDnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = nx.to_numpy_array(G, weight=\"weight\")\n",
        "A = A + np.eye(G.number_of_nodes())"
      ],
      "metadata": {
        "id": "XJm55UjmIE0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "degrees = []\n",
        "for d in G.degree(weight=None):\n",
        "    if d == 0:\n",
        "        degrees.append(0)\n",
        "    else:\n",
        "        degrees.append(d[1]**(-0.5))\n",
        "degrees = np.diag(degrees)"
      ],
      "metadata": {
        "id": "U9R765b4LU3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.eye(G.number_of_nodes()) # Features are just identity matrix\n",
        "A_hat = (degrees@A@degrees)\n",
        "f = X # (n X n) X (n X n) x (n X n) X (n X n) input of net"
      ],
      "metadata": {
        "id": "IZv3d56NMJ0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_hat.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0YDSERlx6JG",
        "outputId": "682f4624-8f63-42d4-f443-4fb9c33b47f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7959, 7959)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = ArgumentParser()\n",
        "parser.add_argument(\"--hidden_size_1\", type=int, default=512, help=\"Size of first GCN hidden weights\")\n",
        "parser.add_argument(\"--hidden_size_2\", type=int, default=256, help=\"Size of second GCN hidden weights\")\n",
        "parser.add_argument(\"--num_classes\", type=int, default=66, help=\"Number of prediction classes\")\n",
        "parser.add_argument(\"--test_ratio\", type=float, default=0.2, help=\"Ratio of test to training nodes\")\n",
        "parser.add_argument(\"--num_epochs\", type=int, default=10000, help=\"No of epochs\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.01, help=\"learning rate\")\n",
        "parser.add_argument(\"--model_no\", type=int, default=0, help=\"Model ID\")\n",
        "args = parser.parse_known_args()"
      ],
      "metadata": {
        "id": "PriqjlcJyJ_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_idxs = []\n",
        "for b_id in df_data[\"b\"].unique():\n",
        "    dum = df_data[df_data[\"b\"] == b_id]\n",
        "    if len(dum) >= 4:\n",
        "        test_idxs.extend(list(np.random.choice(dum.index, size=round(args[0].test_ratio*len(dum)), replace=False)))"
      ],
      "metadata": {
        "id": "2Kz2VixmMXhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected = []\n",
        "for i in range(len(df_data)):\n",
        "    if i not in test_idxs:\n",
        "        selected.append(i)"
      ],
      "metadata": {
        "id": "MMVvUqqd0NNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_selected = f[selected]\n",
        "f_selected = torch.from_numpy(f_selected).float()\n",
        "labels_selected = [l for idx, l in enumerate(df_data[\"b\"]) if idx in selected]\n",
        "f_not_selected = f[test_idxs]\n",
        "f_not_selected = torch.from_numpy(f_not_selected).float()\n",
        "labels_not_selected = [l for idx, l in enumerate(df_data[\"b\"]) if idx not in selected]\n",
        "f = torch.from_numpy(f).float().to(device)"
      ],
      "metadata": {
        "id": "r6Wt5Ios0XEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import savetxt\n",
        "\n",
        "savetxt('/content/X.csv', X, delimiter=',')\n",
        "savetxt('/content/A_hat.csv', A_hat, delimiter=',')\n",
        "# torch.save(X, '/content/X.pt')\n",
        "# torch.save(A_hat, '/content/A_hat.pt')"
      ],
      "metadata": {
        "id": "AiJwIfSFsg1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = gcn(torch.tensor(X.shape[1]).to(device), torch.tensor(A_hat).clone().to(device), args[0]).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=args[0].lr)"
      ],
      "metadata": {
        "id": "6n4w7_Jryqv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04827e91-235e-47a9-9db5-9c827e09886a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-1a266da03376>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.A_hat = torch.tensor(A_hat, requires_grad=False).float()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My1IaoSgx7Dc",
        "outputId": "97f97f24-f60b-4a77-915b-b0617f578c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gcn(\n",
              "  (fc1): Linear(in_features=256, out_features=66, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(output, labels_e):\n",
        "  _, labels = output.max(1); labels = labels.numpy()\n",
        "  return sum([(e-1) for e in labels_e] == labels)/len(labels)"
      ],
      "metadata": {
        "id": "NhjS79Pa1KEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_per_epoch=[]\n",
        "evaluation_trained=[]\n",
        "evaluation_untrained=[]\n",
        "outputs=[]\n",
        "\n",
        "for e in range(args[0].num_epochs):\n",
        "  net.train()\n",
        "  optimizer.zero_grad()\n",
        "  output = net(f).to(device)\n",
        "  loss = criterion(output[selected], (torch.tensor(labels_selected).long() -1).to(device))\n",
        "  losses_per_epoch.append(loss.item())\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if e % 50 == 0:\n",
        "    ### Evaluate other untrained nodes and check accuracy of labelling\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        pred_labels = net(f)\n",
        "        trained_accuracy = evaluate(output[selected].to('cpu'), labels_selected)\n",
        "        untrained_accuracy = evaluate(pred_labels[test_idxs].to('cpu'), labels_not_selected)\n",
        "    evaluation_trained.append((e, trained_accuracy))\n",
        "    evaluation_untrained.append((e, untrained_accuracy))\n",
        "    outputs.append(output)\n",
        "    print(\"[Epoch %d]: Evaluation accuracy of trained nodes: %.7f\" % (e, trained_accuracy))\n",
        "    print(\"[Epoch %d]: Evaluation accuracy of test nodes: %.7f\" % (e, untrained_accuracy))\n",
        "    # print(\"Labels of trained nodes: \\n\", output[selected].max(1)[1])\n",
        "    net.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN_pyECizBG-",
        "outputId": "78fffcf4-a530-43f7-fac5-3c8014693859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0]: Evaluation accuracy of trained nodes: 0.9749216\n",
            "[Epoch 0]: Evaluation accuracy of test nodes: 0.4525862\n",
            "[Epoch 50]: Evaluation accuracy of trained nodes: 0.9770115\n",
            "[Epoch 50]: Evaluation accuracy of test nodes: 0.4525862\n",
            "[Epoch 100]: Evaluation accuracy of trained nodes: 0.9801463\n",
            "[Epoch 100]: Evaluation accuracy of test nodes: 0.4525862\n",
            "[Epoch 150]: Evaluation accuracy of trained nodes: 0.9832811\n",
            "[Epoch 150]: Evaluation accuracy of test nodes: 0.4525862\n",
            "[Epoch 200]: Evaluation accuracy of trained nodes: 0.9843260\n",
            "[Epoch 200]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 250]: Evaluation accuracy of trained nodes: 0.9853710\n",
            "[Epoch 250]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 300]: Evaluation accuracy of trained nodes: 0.9864159\n",
            "[Epoch 300]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 350]: Evaluation accuracy of trained nodes: 0.9885057\n",
            "[Epoch 350]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 400]: Evaluation accuracy of trained nodes: 0.9885057\n",
            "[Epoch 400]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 450]: Evaluation accuracy of trained nodes: 0.9895507\n",
            "[Epoch 450]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 500]: Evaluation accuracy of trained nodes: 0.9895507\n",
            "[Epoch 500]: Evaluation accuracy of test nodes: 0.4525862\n",
            "[Epoch 550]: Evaluation accuracy of trained nodes: 0.9895507\n",
            "[Epoch 550]: Evaluation accuracy of test nodes: 0.4525862\n",
            "[Epoch 600]: Evaluation accuracy of trained nodes: 0.9895507\n",
            "[Epoch 600]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 650]: Evaluation accuracy of trained nodes: 0.9905956\n",
            "[Epoch 650]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 700]: Evaluation accuracy of trained nodes: 0.9905956\n",
            "[Epoch 700]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 750]: Evaluation accuracy of trained nodes: 0.9916405\n",
            "[Epoch 750]: Evaluation accuracy of test nodes: 0.4525862\n",
            "[Epoch 800]: Evaluation accuracy of trained nodes: 0.9916405\n",
            "[Epoch 800]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 850]: Evaluation accuracy of trained nodes: 0.9926855\n",
            "[Epoch 850]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 900]: Evaluation accuracy of trained nodes: 0.9926855\n",
            "[Epoch 900]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 950]: Evaluation accuracy of trained nodes: 0.9937304\n",
            "[Epoch 950]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 1000]: Evaluation accuracy of trained nodes: 0.9937304\n",
            "[Epoch 1000]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 1050]: Evaluation accuracy of trained nodes: 0.9937304\n",
            "[Epoch 1050]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 1100]: Evaluation accuracy of trained nodes: 0.9937304\n",
            "[Epoch 1100]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 1150]: Evaluation accuracy of trained nodes: 0.9937304\n",
            "[Epoch 1150]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 1200]: Evaluation accuracy of trained nodes: 0.9947753\n",
            "[Epoch 1200]: Evaluation accuracy of test nodes: 0.4525862\n",
            "[Epoch 1250]: Evaluation accuracy of trained nodes: 0.9947753\n",
            "[Epoch 1250]: Evaluation accuracy of test nodes: 0.4525862\n",
            "[Epoch 1300]: Evaluation accuracy of trained nodes: 0.9947753\n",
            "[Epoch 1300]: Evaluation accuracy of test nodes: 0.4525862\n",
            "[Epoch 1350]: Evaluation accuracy of trained nodes: 0.9947753\n",
            "[Epoch 1350]: Evaluation accuracy of test nodes: 0.4525862\n",
            "[Epoch 1400]: Evaluation accuracy of trained nodes: 0.9947753\n",
            "[Epoch 1400]: Evaluation accuracy of test nodes: 0.4525862\n",
            "[Epoch 1450]: Evaluation accuracy of trained nodes: 0.9947753\n",
            "[Epoch 1450]: Evaluation accuracy of test nodes: 0.4525862\n",
            "[Epoch 1500]: Evaluation accuracy of trained nodes: 0.9947753\n",
            "[Epoch 1500]: Evaluation accuracy of test nodes: 0.4568966\n",
            "[Epoch 1550]: Evaluation accuracy of trained nodes: 0.9947753\n",
            "[Epoch 1550]: Evaluation accuracy of test nodes: 0.4568966\n",
            "[Epoch 1600]: Evaluation accuracy of trained nodes: 0.9958203\n",
            "[Epoch 1600]: Evaluation accuracy of test nodes: 0.4568966\n",
            "[Epoch 1650]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 1650]: Evaluation accuracy of test nodes: 0.4568966\n",
            "[Epoch 1700]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 1700]: Evaluation accuracy of test nodes: 0.4568966\n",
            "[Epoch 1750]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 1750]: Evaluation accuracy of test nodes: 0.4568966\n",
            "[Epoch 1800]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 1800]: Evaluation accuracy of test nodes: 0.4568966\n",
            "[Epoch 1850]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 1850]: Evaluation accuracy of test nodes: 0.4568966\n",
            "[Epoch 1900]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 1900]: Evaluation accuracy of test nodes: 0.4568966\n",
            "[Epoch 1950]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 1950]: Evaluation accuracy of test nodes: 0.4568966\n",
            "[Epoch 2000]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 2000]: Evaluation accuracy of test nodes: 0.4568966\n",
            "[Epoch 2050]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 2050]: Evaluation accuracy of test nodes: 0.4568966\n",
            "[Epoch 2100]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 2100]: Evaluation accuracy of test nodes: 0.4568966\n",
            "[Epoch 2150]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 2150]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 2200]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 2200]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 2250]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 2250]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 2300]: Evaluation accuracy of trained nodes: 0.9968652\n",
            "[Epoch 2300]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 2350]: Evaluation accuracy of trained nodes: 0.9979101\n",
            "[Epoch 2350]: Evaluation accuracy of test nodes: 0.4482759\n",
            "[Epoch 2400]: Evaluation accuracy of trained nodes: 0.9979101\n",
            "[Epoch 2400]: Evaluation accuracy of test nodes: 0.4439655\n",
            "[Epoch 2450]: Evaluation accuracy of trained nodes: 0.9979101\n",
            "[Epoch 2450]: Evaluation accuracy of test nodes: 0.4439655\n",
            "[Epoch 2500]: Evaluation accuracy of trained nodes: 0.9979101\n",
            "[Epoch 2500]: Evaluation accuracy of test nodes: 0.4439655\n",
            "[Epoch 2550]: Evaluation accuracy of trained nodes: 0.9989551\n",
            "[Epoch 2550]: Evaluation accuracy of test nodes: 0.4439655\n",
            "[Epoch 2600]: Evaluation accuracy of trained nodes: 0.9989551\n",
            "[Epoch 2600]: Evaluation accuracy of test nodes: 0.4439655\n",
            "[Epoch 2650]: Evaluation accuracy of trained nodes: 0.9989551\n",
            "[Epoch 2650]: Evaluation accuracy of test nodes: 0.4439655\n",
            "[Epoch 2700]: Evaluation accuracy of trained nodes: 0.9989551\n",
            "[Epoch 2700]: Evaluation accuracy of test nodes: 0.4396552\n",
            "[Epoch 2750]: Evaluation accuracy of trained nodes: 0.9989551\n",
            "[Epoch 2750]: Evaluation accuracy of test nodes: 0.4396552\n",
            "[Epoch 2800]: Evaluation accuracy of trained nodes: 0.9989551\n",
            "[Epoch 2800]: Evaluation accuracy of test nodes: 0.4396552\n",
            "[Epoch 2850]: Evaluation accuracy of trained nodes: 0.9989551\n",
            "[Epoch 2850]: Evaluation accuracy of test nodes: 0.4396552\n",
            "[Epoch 2900]: Evaluation accuracy of trained nodes: 0.9989551\n",
            "[Epoch 2900]: Evaluation accuracy of test nodes: 0.4396552\n",
            "[Epoch 2950]: Evaluation accuracy of trained nodes: 0.9989551\n",
            "[Epoch 2950]: Evaluation accuracy of test nodes: 0.4353448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# torch.save(net.state_dict(), '/content/textgcn.pt')\n",
        "# downloaded = files.download('/content/textgcn.pt')"
      ],
      "metadata": {
        "id": "HCTSMjhfMqUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uploaded = files.upload()\n",
        "loaded = torch.load('/content/textgcn.pt')"
      ],
      "metadata": {
        "id": "_EdyRZa7zKnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded\n",
        "\n",
        "# model = torch.load(PATH)"
      ],
      "metadata": {
        "id": "HkjxIKkbNTbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z1-pBVebOnYw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}